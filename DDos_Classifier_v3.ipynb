{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urkkDPRZ3kiK"
   },
   "source": [
    "# Load and Train the desire testing dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "Be-80uIesm83",
    "outputId": "85dbd8ab-571b-45b3-d860-30ba55cb005f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# data = pd.read_csv('https://ddosciu.s3.us-east-2.amazonaws.com/CSVs/UNB/Portmap.csv')\n",
    "# data = pd.read_csv('https://unsw-my.sharepoint.com/:x:/r/personal/z5025758_ad_unsw_edu_au/_layouts/15/Doc.aspx?sourcedoc=%7B2A810F6A-CC3D-4D98-909E-37489D8DAF98%7D&file=UNSW_NB15_testing-set.csv&action=default&mobileredirect=true')\n",
    "data = pd.read_csv(\"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/Train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zuk419jUmX89",
    "outputId": "bda4f1ac-da7f-475e-eb4b-39bfd6c52cf3"
   },
   "outputs": [],
   "source": [
    "set(data[' Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s26T5AoCs-FD",
    "outputId": "d6b04ae4-bfc0-4cbe-ec21-36c97fc023ac"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEtopw7E5fZ7"
   },
   "source": [
    "Total Fwd Packets (feature 9), Total Backward Packets (feature 10), Flow Packets/s (feature 43):, Flow Bytes/s (feature 21), Protocol (feature 6): , Flow Duration (feature 8):, Packet Length Features (features 45, 46, 47, 48):, Flag Counts (features 50-57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXCZUJgzCDnT"
   },
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaPc6CqjsuEt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode categorical features (e.g., IP addresses)\n",
    "encoder = LabelEncoder()\n",
    "data[' Source IP'] = encoder.fit_transform(data[' Source IP'])\n",
    "data[' Destination IP'] = encoder.fit_transform(data[' Destination IP'])\n",
    "data[' Timestamp'] = pd.to_datetime(data[' Timestamp']).astype(np.int64)\n",
    "data[' Label'] = encoder.fit_transform(data[' Label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey0d206H4_1C"
   },
   "outputs": [],
   "source": [
    "# Inspect the mapping between original labels and encoded numbers\n",
    "print(\"Mapping between original labels and encoded numbers:\")\n",
    "for label, encoded_label in zip(encoder.classes_, encoder.transform(encoder.classes_)):\n",
    "    print(f\"{label}: {encoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5gVNyArtHrx"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns in the CVs's\n",
    "try:\n",
    "  data = data.drop(['Unnamed: 0', 'Flow ID', 'SimillarHTTP'], axis=1)\n",
    "  data = data.drop([' Fwd Header Length.1'], axis=1)\n",
    "except:\n",
    "  print('Columns are dropped already')\n",
    "\n",
    "  # Drop unnecessary columns in the CVs's\n",
    "try:\n",
    "  data = data.drop([' Fwd Header Length.1'], axis=1)\n",
    "except:\n",
    "  print('Columns are dropped already')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM8SMfa_tIBn"
   },
   "outputs": [],
   "source": [
    "# Set the desired number of majority class samples\n",
    "num_majority_samples = 5000 #100000\n",
    "\n",
    "# Get the minority class label\n",
    "minority_class = data[' Label'].value_counts().idxmin()\n",
    "\n",
    "# Separate majority and minority class samples\n",
    "majority_samples = data[data[' Label'] != minority_class]\n",
    "minority_samples = data[data[' Label'] == minority_class]\n",
    "\n",
    "# Sample the majority class samples\n",
    "majority_samples_sampled = majority_samples.sample(num_majority_samples, random_state=42)\n",
    "\n",
    "# Combine the sampled majority class samples and minority class samples\n",
    "balanced_data = pd.concat([majority_samples_sampled, minority_samples], axis=0)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chSrKxsLbmDO"
   },
   "outputs": [],
   "source": [
    "num_unique_labels = balanced_data[' Label'].nunique()\n",
    "print(\"Number of unique labels in the balanced data:\", num_unique_labels)\n",
    "label_frequency = balanced_data[' Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the balanced data:\")\n",
    "print(label_frequency)\n",
    "label_frequency_data = data[' Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the original data:\")\n",
    "print(label_frequency_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaWoy5NWtfLd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataset is named 'data'\n",
    "correlation = balanced_data[' Inbound'].corr(balanced_data[' Label'])\n",
    "print(\"Correlation between 'Inbound' and 'Label':\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gd0rO7VHtk2b"
   },
   "outputs": [],
   "source": [
    "balanced_data[' Inbound'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ZwG01pnhtlxb",
    "outputId": "5b78e208-2140-44d4-e96f-9b4980e19fde"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is named 'df'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=balanced_data, x=' Inbound', hue=' Label')\n",
    "plt.title('Frequency of Inbound with Label as Hue')\n",
    "plt.xlabel('Inbound')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RS_pAjXntpyn"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is named 'df'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=balanced_data, x=' Label', hue=' Inbound')\n",
    "plt.title('Frequency of Label with Inbound as Hue')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgdhLrZVtp5q"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming your dataframe is named 'df'\n",
    "inbound_label_counts = balanced_data.groupby([' Inbound', ' Label']).size().reset_index(name='Count')\n",
    "total_counts = balanced_data.groupby([' Inbound']).size().reset_index(name='Total_Count')\n",
    "\n",
    "inbound_label_counts = inbound_label_counts.merge(total_counts, on=' Inbound')\n",
    "inbound_label_counts['Percentage'] = (inbound_label_counts['Count'] / inbound_label_counts['Total_Count']) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=inbound_label_counts, x=' Inbound', y='Percentage', hue=' Label')\n",
    "plt.title('Frequency of Inbound with Label as Hue (Percentage)')\n",
    "plt.xlabel('Inbound')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "RRHhW20etyxS",
    "outputId": "4a727695-b1c8-4f57-e36b-290222349fe1"
   },
   "outputs": [],
   "source": [
    "inbound_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hly-X2lxty9k"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X = balanced_data.drop([' Inbound',' Label'], axis=1)\n",
    "y = balanced_data[' Label']\n",
    "\n",
    "selected_columns = [\n",
    "    ' Total Fwd Packets',\n",
    "    ' Total Backward Packets',\n",
    "    ' Flow Packets/s',\n",
    "    'Flow Bytes/s',\n",
    "    ' Protocol',\n",
    "    # ' Flow Duration',\n",
    "    ' SYN Flag Count',\n",
    "    ' RST Flag Count',\n",
    "    ' PSH Flag Count',\n",
    "    ' ACK Flag Count',\n",
    "    ' URG Flag Count',\n",
    "    ' CWE Flag Count',\n",
    "    ' ECE Flag Count',\n",
    "    'Fwd Packets/s',\n",
    "    ' Bwd Packets/s',\n",
    "    ' Min Packet Length',\n",
    "    ' Max Packet Length',\n",
    "    ' Packet Length Mean',\n",
    "    ' Packet Length Std',\n",
    "    ' Packet Length Variance'\n",
    "]\n",
    "\n",
    "# Selecting only the columns of interest\n",
    "X_specific = X[selected_columns]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_specific, y, test_size=0.2,stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khLWdYqZ78lu",
    "outputId": "2b8e2746-711c-4182-c1c5-d7717410bbbc"
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9Ssmjv1t5_E",
    "outputId": "15d23456-cf6d-42df-e548-0639e6fc6b9c"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DarY50-t9TA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_data_issues(data):\n",
    "    if data.isnull().values.any():\n",
    "        print(\"There are NaN values in the dataset.\")\n",
    "    # Check for infinite values\n",
    "    if np.isinf(data).values.any():\n",
    "        print(\"There are infinite values in the dataset.\")\n",
    "    # Check for extremely large values\n",
    "    max_value = data.max().max()\n",
    "    if max_value > np.finfo(np.float64).max:\n",
    "        print(f\"There are values too large for dtype('float64') in the dataset. Max value: {max_value}\")\n",
    "    # Check for extremely small values\n",
    "    min_value = data.min().min()\n",
    "    if min_value < np.finfo(np.float64).min:\n",
    "        print(f\"There are values too small for dtype('float64') in the dataset. Min value: {min_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfnrV4BCt9nH",
    "outputId": "f0bfafa5-2f89-4d62-967c-85ce452b901d"
   },
   "outputs": [],
   "source": [
    "# Check the training and testing data for any issues\n",
    "check_data_issues(X_train)\n",
    "check_data_issues(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bHXh2LSuLH1"
   },
   "outputs": [],
   "source": [
    "# Replace infinity values with NaN\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW2E-Hyy6RhG",
    "outputId": "233c61b6-6298-4efd-f2f8-1ae7408f88ce"
   },
   "outputs": [],
   "source": [
    "X_train.columns\n",
    "column_names =X_train.columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXKEKw8guSJj"
   },
   "outputs": [],
   "source": [
    "# Impute NaN values with the mean of the corresponding column\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Now, standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXJO_fNc41Lv",
    "outputId": "75f8b48f-9085-4e3d-b86d-42f2e60e30f4"
   },
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XTWVrQXmgA2r",
    "outputId": "fc9b89fc-3f78-49ed-c2fd-854ece3142bd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump\n",
    "\n",
    "# Create an empty DataFrame with the desired columns\n",
    "model_comparison = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "saved_models = {}  # Dictionary to store saved models\n",
    "\n",
    "\n",
    "# Train and evaluate machine learning models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC()\n",
    "}\n",
    "print(\"X shape\",X_train.shape)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Append the metrics to the DataFrame\n",
    "    model_comparison = model_comparison._append({\"Model\": name, \"Accuracy\": accuracy, \"F1 Score\": f1}, ignore_index=True)\n",
    "\n",
    "    # Save the trained model weight\n",
    "    filename = f\"{name}_model.joblib\"\n",
    "    dump(model, filename)\n",
    "    saved_models[name] = filename  # Store the filename for later use in the tested DataSets\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Model saved as: {filename}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "display(model_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "IKZrkjJkgEFD",
    "outputId": "382db3d8-ad58-4687-80fd-d2d72f112354"
   },
   "outputs": [],
   "source": [
    "# Display the comparison DataFrame in the splited train data set \n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Wtvq0IxFsRSJ",
    "outputId": "d01ce1d9-ca99-455d-bacc-921f4bcd7219"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the DataFrame to store model comparison\n",
    "model_comparison = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "\n",
    "# Initialize dictionary to store confusion matrices\n",
    "confusion_matrices = {}\n",
    "\n",
    "# Initialize lists to store accuracy and F1 score for each model\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over each loaded model\n",
    "for name, model in models.items():\n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "\n",
    "    # Append results to model comparison DataFrame\n",
    "    model_comparison = model_comparison._append({\"Model\": name, \"Accuracy\": accuracy, \"F1 Score\": f1}, ignore_index=True)\n",
    "\n",
    "    # Append accuracy and F1 score to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print model evaluation metrics and confusion matrix\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plotting accuracy scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(model_comparison['Model'], accuracy_scores, color='skyblue')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plotting F1 scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(model_comparison['Model'], f1_scores, color='orange')\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Displaying the model comparison DataFrame\n",
    "print(\"Model Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Plot confusion matrices for each model using Seaborn\n",
    "for name, cm in confusion_matrices.items():\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZP--5ZNSlLw"
   },
   "source": [
    "# Loading Saved model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FryA8a9igzO"
   },
   "outputs": [],
   "source": [
    "#Load the saved model\n",
    "from joblib import load\n",
    "\n",
    "# Load each model individually\n",
    "# Give path of each model here\n",
    "logistic_regression_model = load(\"Logistic Regression_model.joblib\")\n",
    "random_forest_model = load(\"Random Forest_model.joblib\")\n",
    "svm_model = load(\"Support Vector Machine_model.joblib\")\n",
    "\n",
    "# Store the loaded models in a dictionary\n",
    "loaded_models = {\n",
    "    \"Logistic Regression\": logistic_regression_model,\n",
    "    \"Random Forest\": random_forest_model,\n",
    "    \"Support Vector Machine\": svm_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrKkHH-11HZK",
    "outputId": "6090d604-ef55-4694-9f47-2c34fdc51ee0"
   },
   "outputs": [],
   "source": [
    "loaded_models.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nitxogSZ1yzg"
   },
   "source": [
    "##Preprocessing For Testing DATA Change Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGNzsOc3ujsD",
    "outputId": "53ea516d-f041-4ef3-ffc8-edaaa68eecb8"
   },
   "outputs": [],
   "source": [
    "feature_columns = X_specific.columns\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENgTyePU2MDI"
   },
   "outputs": [],
   "source": [
    "\n",
    "# In case i dont have book.xlsx I have manually place the columns mapping here too' where keys are testing columns and values are corresponding training columns\n",
    "\n",
    "column_mapping = {'Flow ID': 'Flow ID',\n",
    " 'Src IP': ' Source IP',\n",
    " 'Src Port': ' Source Port',\n",
    " 'Dst IP': ' Destination IP',\n",
    " 'Dst Port': ' Destination Port',\n",
    " 'Protocol': ' Protocol',\n",
    " 'Timestamp': ' Timestamp',\n",
    " 'Flow Duration': ' Flow Duration',\n",
    " 'Tot Fwd Pkts': ' Total Fwd Packets',\n",
    " 'Tot Bwd Pkts': ' Total Backward Packets',\n",
    " 'TotLen Fwd Pkts': 'Total Length of Fwd Packets',\n",
    " 'TotLen Bwd Pkts': ' Total Length of Bwd Packets',\n",
    " 'Fwd Pkt Len Max': ' Fwd Packet Length Max',\n",
    " 'Fwd Pkt Len Min': ' Fwd Packet Length Min',\n",
    " 'Fwd Pkt Len Mean': ' Fwd Packet Length Mean',\n",
    " 'Fwd Pkt Len Std': ' Fwd Packet Length Std',\n",
    " 'Bwd Pkt Len Max': 'Bwd Packet Length Max',\n",
    " 'Bwd Pkt Len Min': ' Bwd Packet Length Min',\n",
    " 'Bwd Pkt Len Mean': ' Bwd Packet Length Mean',\n",
    " 'Bwd Pkt Len Std': ' Bwd Packet Length Std',\n",
    " 'Flow Byts/s': 'Flow Bytes/s',\n",
    " 'Flow Pkts/s': ' Flow Packets/s',\n",
    " 'Flow IAT Mean': ' Flow IAT Mean',\n",
    " 'Flow IAT Std': ' Flow IAT Std',\n",
    " 'Flow IAT Max': ' Flow IAT Max',\n",
    " 'Flow IAT Min': ' Flow IAT Min',\n",
    " 'Fwd IAT Tot': 'Fwd IAT Total',\n",
    " 'Fwd IAT Mean': ' Fwd IAT Mean',\n",
    " 'Fwd IAT Std': ' Fwd IAT Std',\n",
    " 'Fwd IAT Max': ' Fwd IAT Max',\n",
    " 'Fwd IAT Min': ' Fwd IAT Min',\n",
    " 'Bwd IAT Tot': 'Bwd IAT Total',\n",
    " 'Bwd IAT Mean': ' Bwd IAT Mean',\n",
    " 'Bwd IAT Std': ' Bwd IAT Std',\n",
    " 'Bwd IAT Max': ' Bwd IAT Max',\n",
    " 'Bwd IAT Min': ' Bwd IAT Min',\n",
    " 'Fwd PSH Flags': 'Fwd PSH Flags',\n",
    " 'Bwd PSH Flags': ' Bwd PSH Flags',\n",
    " 'Fwd URG Flags': ' Fwd URG Flags',\n",
    " 'Bwd URG Flags': ' Bwd URG Flags',\n",
    " 'Fwd Header Len': ' Fwd Header Length',\n",
    " 'Bwd Header Len': ' Bwd Header Length',\n",
    " 'Fwd Pkts/s': 'Fwd Packets/s',\n",
    " 'Bwd Pkts/s': ' Bwd Packets/s',\n",
    " 'Pkt Len Min': ' Min Packet Length',\n",
    " 'Pkt Len Max': ' Max Packet Length',\n",
    " 'Pkt Len Mean': ' Packet Length Mean',\n",
    " 'Pkt Len Std': ' Packet Length Std',\n",
    " 'Pkt Len Var': ' Packet Length Variance',\n",
    " 'FIN Flag Cnt': 'FIN Flag Count',\n",
    " 'SYN Flag Cnt': ' SYN Flag Count',\n",
    " 'RST Flag Cnt': ' RST Flag Count',\n",
    " 'PSH Flag Cnt': ' PSH Flag Count',\n",
    " 'ACK Flag Cnt': ' ACK Flag Count',\n",
    " 'URG Flag Cnt': ' URG Flag Count',\n",
    " 'CWE Flag Count': ' CWE Flag Count',\n",
    " 'ECE Flag Cnt': ' ECE Flag Count',\n",
    " 'Down/Up Ratio': ' Down/Up Ratio',\n",
    " 'Pkt Size Avg': ' Average Packet Size',\n",
    " 'Fwd Seg Size Avg': ' Avg Fwd Segment Size',\n",
    " 'Bwd Seg Size Avg': ' Avg Bwd Segment Size',\n",
    " 'Fwd Byts/b Avg': 'Fwd Avg Bytes/Bulk',\n",
    " 'Fwd Pkts/b Avg': ' Fwd Avg Packets/Bulk',\n",
    " 'Fwd Blk Rate Avg': ' Fwd Avg Bulk Rate',\n",
    " 'Bwd Byts/b Avg': ' Bwd Avg Bytes/Bulk',\n",
    " 'Bwd Pkts/b Avg': ' Bwd Avg Packets/Bulk',\n",
    " 'Bwd Blk Rate Avg': 'Bwd Avg Bulk Rate',\n",
    " 'Subflow Fwd Pkts': 'Subflow Fwd Packets',\n",
    " 'Subflow Fwd Byts': ' Subflow Fwd Bytes',\n",
    " 'Subflow Bwd Pkts': ' Subflow Bwd Packets',\n",
    " 'Subflow Bwd Byts': ' Subflow Bwd Bytes',\n",
    " 'Init Fwd Win Byts': 'Init_Win_bytes_forward',\n",
    " 'Init Bwd Win Byts': ' Init_Win_bytes_backward',\n",
    " 'Fwd Act Data Pkts': ' act_data_pkt_fwd',\n",
    " 'Fwd Seg Size Min': ' min_seg_size_forward',\n",
    " 'Active Mean': 'Active Mean',\n",
    " 'Active Std': ' Active Std',\n",
    " 'Active Max': ' Active Max',\n",
    " 'Active Min': ' Active Min',\n",
    " 'Idle Mean': 'Idle Mean',\n",
    " 'Idle Std': ' Idle Std',\n",
    " 'Idle Max': ' Idle Max',\n",
    " 'Idle Min': ' Idle Min',\n",
    " 'Label': 'Label'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D7xkdrRRujD"
   },
   "source": [
    "# Loading target dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8rGrRtwi0uM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the new dataset we going to test the model on:\n",
    "url = \"https://ddosciu.s3.us-east-2.amazonaws.com/PCAPs/Test.csv\"\n",
    "#Test1\n",
    "url1 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/HTTP_Ddos.pcap_Flow.csv\"\n",
    "url2 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.TCP.syn.optionallyACK.optionallysamePort.pcapng_Flow.csv\"\n",
    "url3 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.TCP.reflection.SYNACK.pcap_Flow.csv\"\n",
    "url4 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.UDP.bacnet.IOT.37810.pcapng_Flow.csv\"\n",
    "url5 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/1.pcap_Flow.csv\"\n",
    "\n",
    "url6 =\"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.dns.RRSIG.fragmented.pcap_Flow.csv\"\n",
    "url7 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.UDP.isakmp.pcap_Flow.csv\"\n",
    "url8 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/pkt.ICMP.largeempty.pcap_Flow.csv\"\n",
    "url9 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/pkt.TCP.DOMINATE.syn.ecn.cwr.pcapng_Flow.csv\"\n",
    "url10 = \"/content/NormalTraffic2.pcap_Flow.csv\"\n",
    "url11= \"https://ddosciu.s3.us-east-2.amazonaws.com/IC_Taraf.root.1.pcap_Flow.csv\"\n",
    "url12= \"https://ddosciu.s3.us-east-2.amazonaws.com/1.csv\"\n",
    "url13= \"https://ddosciu.s3.us-east-2.amazonaws.com/21.csv\"\n",
    "url14 = \"/content/1.csv\"\n",
    "\n",
    "\n",
    "new_data = pd.read_csv(url13)\n",
    "\n",
    "# Apply the column mapping to the testing data\n",
    "new_data = new_data.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqceS9yXcfUA",
    "outputId": "b1a56879-71eb-4090-af17-bf319e816013"
   },
   "outputs": [],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osEj0ittA4Ts",
    "outputId": "ac6c3c66-8508-4d68-f8aa-994569f5cb0d"
   },
   "outputs": [],
   "source": [
    "len(X.columns),len(new_data.columns)\n",
    "# Check for missing columns in new_data compared to train data X\n",
    "missing_columns = set(X.columns) - set(new_data.columns)\n",
    "\n",
    "# Check for missing columns in train data X compared to new_data\n",
    "extra_columns = set(new_data.columns) - set(X.columns)\n",
    "\n",
    "print(\"Columns missing in new_data:\", missing_columns)\n",
    "print(\"Columns extra in new_data:\", extra_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRgQReS8QGkK"
   },
   "outputs": [],
   "source": [
    "# Same Preprocessing for new dataset as i did with training dataset\n",
    "# Drop irrelevant columns if necessary\n",
    "columns_to_drop = ['Unnamed: 0', 'Flow ID', 'SimillarHTTP']\n",
    "\n",
    "# Filter out columns that are not present in the DataFrame\n",
    "columns_to_drop_existing = [column for column in columns_to_drop if column in new_data.columns]\n",
    "\n",
    "# Drop the existing columns\n",
    "if columns_to_drop_existing:\n",
    "    new_data = new_data.drop(columns_to_drop_existing, axis=1)\n",
    "else:\n",
    "    print(\"All columns to drop are not present in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "6S-vpcNyCqcl",
    "outputId": "944f8dc1-8116-49fb-ae39-0f1d4eecc30b"
   },
   "outputs": [],
   "source": [
    "new_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaMqYH8C_XAh",
    "outputId": "73f84b7e-d671-4a54-bcd2-8dfc04d7dd96"
   },
   "outputs": [],
   "source": [
    "def standardize_timestamp_format(data, column_name=' Timestamp'):\n",
    "    if column_name in data.columns:\n",
    "        try:\n",
    "            # Trying to convert the 'Timestamp' column to datetime using the first row to infer the format\n",
    "            data[column_name] = pd.to_datetime(data[column_name])\n",
    "        except ValueError:\n",
    "            # If ValueError occurs (e.g., due to a different format), try to parse the timestamp with multiple formats\n",
    "            data[column_name] = pd.to_datetime(data[column_name], errors='coerce', format='%d/%m/%Y %I:%M:%S %p')\n",
    "            data[column_name].fillna(pd.to_datetime(data[column_name], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f'), inplace=True)\n",
    "    return data\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = LabelEncoder()\n",
    "new_data[' Source IP'] = encoder.fit_transform(new_data[' Source IP'])\n",
    "new_data[' Destination IP'] = encoder.fit_transform(new_data[' Destination IP'])\n",
    "# new_data[' Label'] = encoder.fit_transform(new_data[' Label'])\n",
    "\n",
    "# Standardize the format of the 'Timestamp' column in the testing data\n",
    "new_data = standardize_timestamp_format(new_data)\n",
    "\n",
    "# Convert 'Timestamp' to datetime and then to int64\n",
    "new_data[' Timestamp'] = pd.to_datetime(new_data[' Timestamp']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xpQU2yfiNcb"
   },
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = ['Inbound','Label']\n",
    "\n",
    "# Filter out columns that are not present in the DataFrame\n",
    "columns_to_drop_existing = [column for column in columns_to_drop if column in new_data.columns]\n",
    "# Drop the existing columns if there are any, otherwise use the original DataFrame\n",
    "if columns_to_drop_existing:\n",
    "    X_new = new_data.drop(columns_to_drop_existing, axis=1)\n",
    "else:\n",
    "    print(\"All columns to drop are not present in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hsdj1VWT_gtK"
   },
   "outputs": [],
   "source": [
    "X_new = X_new[X_specific.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w3vyogQb8iE"
   },
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_columns = X_new.select_dtypes(exclude=np.number).columns\n",
    "# print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "# Convert non-numeric columns to numeric format\n",
    "for column in non_numeric_columns:\n",
    "    X_new[column] = pd.to_numeric(X_new[column], errors='coerce')\n",
    "\n",
    "X_new.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_new = imputer.fit_transform(X_new)\n",
    "scaler = StandardScaler()\n",
    "X_new = scaler.fit_transform(X_new)\n",
    "\n",
    "#Labels for comparison (They are not always available for some datasets)\n",
    "# y_new = new_data[' Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz2fc-oDBfw_"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXtxYcPwmnnR",
    "outputId": "ac65fbda-ad47-42a9-8e2c-b4af7dcc628d"
   },
   "outputs": [],
   "source": [
    "# Now, i can use the loaded models to make predictions on the preprocessed new dataset\n",
    "predictions = {}\n",
    "for name, model in loaded_models.items():\n",
    "    predictions[name] = model.predict(X_new)\n",
    "\n",
    "# Print predictions for each model\n",
    "for name, preds in predictions.items():\n",
    "    print(f\"Predictions using {name}: {preds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcW6mhPs7cb1",
    "outputId": "af28e849-9591-42ba-8512-0c0979164cd9"
   },
   "outputs": [],
   "source": [
    "# Define a mapping from encoded labels to original labels\n",
    "label_mapping = {0: 'BENIGN', 1: 'LDAP', 2: 'NetBIOS'}\n",
    "\n",
    "# Now, i can use the loaded models to make predictions on the preprocessed new dataset\n",
    "predictions = {}\n",
    "\n",
    "# Iterate through each loaded model\n",
    "for name, model in loaded_models.items():\n",
    "    # Make predictions using the current model\n",
    "    preds = model.predict(X_new)\n",
    "    # Map encoded predictions to their original labels\n",
    "    mapped_preds = [label_mapping[pred] for pred in preds]\n",
    "    # Store the mapped predictions\n",
    "    predictions[name] = mapped_preds\n",
    "\n",
    "# Print mapped predictions for each model\n",
    "for name, preds in predictions.items():\n",
    "    print(f\"Predictions using {name}: {preds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Q_VVt80c9hIN",
    "outputId": "596cdc11-1ec6-4e4c-9246-a01dbd0deffb"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame\n",
    "all_predictions_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a list to store model names\n",
    "model_names = []\n",
    "\n",
    "# Iterate over each loaded model\n",
    "for name, model in loaded_models.items():\n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_new)\n",
    "\n",
    "    # Map the predicted labels\n",
    "    mapped_predictions = [label_mapping[label] for label in y_pred]\n",
    "\n",
    "    # Add the model name to the list\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Create a DataFrame for predicted labels\n",
    "    predicted_labels_df = pd.DataFrame({'Predicted_Label': y_pred, 'Mapped_Prediction': mapped_predictions})\n",
    "\n",
    "    # Add the DataFrame to the overall DataFrame\n",
    "    all_predictions_df = pd.concat([all_predictions_df, predicted_labels_df], axis=1)\n",
    "\n",
    "# Set the column names for the first row\n",
    "all_predictions_df.columns = pd.MultiIndex.from_product([model_names, ['Predicted_Label', 'Mapped_Prediction']])\n",
    "\n",
    "# Print the DataFrame\n",
    "all_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eG2zx34aBmDG",
    "outputId": "664bb3d7-9004-4e0e-d57d-9aa639d11d83"
   },
   "outputs": [],
   "source": [
    "# Define a mapping from encoded labels to original labels\n",
    "label_mapping = {0: 'No Attack'}\n",
    "\n",
    "# Now, you can use the loaded models to make predictions on the preprocessed new dataset\n",
    "predictions = {}\n",
    "\n",
    "# Iterate through each loaded model\n",
    "for name, model in loaded_models.items():\n",
    "    # Make predictions using the current model\n",
    "    preds = model.predict(X_new)\n",
    "    # Map encoded predictions to their original labels\n",
    "    mapped_preds = ['No Attack' if pred == 0 else 'DDos Attack' for pred in preds]\n",
    "    # Store the mapped predictions\n",
    "    predictions[name] = mapped_preds\n",
    "\n",
    "# Print mapped predictions for each model\n",
    "for name, preds in predictions.items():\n",
    "    print(f\"Predictions using {name}: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "NKhmYD31BpEO",
    "outputId": "7e072c8e-ca7e-4c81-c6c4-75c222807075"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_predictions_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a list to store model names\n",
    "model_names = []\n",
    "\n",
    "# Iterate over each loaded model\n",
    "for name, model in loaded_models.items():\n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_new)\n",
    "\n",
    "    # Map the predicted labels\n",
    "    mapped_predictions = ['No Attack' if label == 0 else 'Attack' for label in y_pred]\n",
    "\n",
    "    # Add the model name to the list\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Create a DataFrame for predicted labels\n",
    "    predicted_labels_df = pd.DataFrame({'Predicted_Label': y_pred, 'Mapped_Prediction': mapped_predictions})\n",
    "\n",
    "    # Add the DataFrame to the overall DataFrame\n",
    "    all_predictions_df = pd.concat([all_predictions_df, predicted_labels_df], axis=1)\n",
    "\n",
    "# Set the column names for the first row\n",
    "all_predictions_df.columns = pd.MultiIndex.from_product([model_names, ['Predicted_Label', 'Mapped_Prediction']])\n",
    "\n",
    "# Print the DataFrame\n",
    "all_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCUmsOnPHIkf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Savinf model prediction to CSV file\n",
    "all_predictions_df.to_csv('test_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
