{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urkkDPRZ3kiK",
    "tags": []
   },
   "source": [
    "# Load and Train the desire testing dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be-80uIesm83",
    "outputId": "efb70b8e-0adc-4e98-bb4c-75443c2ceb2c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16168/32133707.py:13: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"https://ddosciu.s3.us-east-2.amazonaws.com/Portmap.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# data = pd.read_csv('https://ddosciu.s3.us-east-2.amazonaws.com/CSVs/UNB/Portmap.csv')\n",
    "# data = pd.read_csv('https://unsw-my.sharepoint.com/:x:/r/personal/z5025758_ad_unsw_edu_au/_layouts/15/Doc.aspx?sourcedoc=%7B2A810F6A-CC3D-4D98-909E-37489D8DAF98%7D&file=UNSW_NB15_testing-set.csv&action=default&mobileredirect=true')\n",
    "##CIC-DDoS2019 DataSet\n",
    "data = pd.read_csv(\"https://ddosciu.s3.us-east-2.amazonaws.com/Portmap.csv\")\n",
    "##UNSW-NB15 DataSet\n",
    "data_2 = pd.read_csv(\"https://ddosciu.s3.us-east-2.amazonaws.com/UNSW-NB15_4.csv\")\n",
    "##CIC-IDS2017 DataSet\n",
    "data_3 = pd.read_csv(\"https://ddosciu.s3.us-east-2.amazonaws.com/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvmFf5QQMATU",
    "tags": []
   },
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMPU8utHMMvF",
    "tags": []
   },
   "source": [
    "## Dataset1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "9Q4GgIqjLt1N",
    "outputId": "aed421e2-28f8-4c09-b868-5536dafb26ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>SimillarHTTP</th>\n",
       "      <th>Inbound</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>192.168.50.254-224.0.0.5-0-0-0</td>\n",
       "      <td>192.168.50.254</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 09:18:16.964447</td>\n",
       "      <td>114456999</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>28337.112288</td>\n",
       "      <td>98168.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9529897.25</td>\n",
       "      <td>351582.631269</td>\n",
       "      <td>10001143.0</td>\n",
       "      <td>9048097.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>192.168.50.253-224.0.0.5-0-0-0</td>\n",
       "      <td>192.168.50.253</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 09:18:18.506537</td>\n",
       "      <td>114347504</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>121314.911865</td>\n",
       "      <td>420255.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9493929.75</td>\n",
       "      <td>351541.079539</td>\n",
       "      <td>9978130.0</td>\n",
       "      <td>8820294.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176563</td>\n",
       "      <td>172.217.10.98-192.168.50.6-443-54799-6</td>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>54799</td>\n",
       "      <td>172.217.10.98</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 09:18:18.610576</td>\n",
       "      <td>36435473</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62416.0</td>\n",
       "      <td>62416.0</td>\n",
       "      <td>36373056.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36373056.0</td>\n",
       "      <td>36373056.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50762</td>\n",
       "      <td>172.217.7.2-192.168.50.6-443-54800-6</td>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>54800</td>\n",
       "      <td>172.217.7.2</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 09:18:18.610579</td>\n",
       "      <td>36434705</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62413.0</td>\n",
       "      <td>62413.0</td>\n",
       "      <td>36372291.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36372291.0</td>\n",
       "      <td>36372291.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87149</td>\n",
       "      <td>172.217.10.98-192.168.50.6-443-54801-6</td>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>54801</td>\n",
       "      <td>172.217.10.98</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 09:18:18.610581</td>\n",
       "      <td>36434626</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62409.0</td>\n",
       "      <td>62409.0</td>\n",
       "      <td>36372216.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36372216.0</td>\n",
       "      <td>36372216.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 Flow ID       Source IP  \\\n",
       "0          24          192.168.50.254-224.0.0.5-0-0-0  192.168.50.254   \n",
       "1          26          192.168.50.253-224.0.0.5-0-0-0  192.168.50.253   \n",
       "2      176563  172.217.10.98-192.168.50.6-443-54799-6    192.168.50.6   \n",
       "3       50762    172.217.7.2-192.168.50.6-443-54800-6    192.168.50.6   \n",
       "4       87149  172.217.10.98-192.168.50.6-443-54801-6    192.168.50.6   \n",
       "\n",
       "    Source Port  Destination IP   Destination Port   Protocol  \\\n",
       "0             0       224.0.0.5                  0          0   \n",
       "1             0       224.0.0.5                  0          0   \n",
       "2         54799   172.217.10.98                443          6   \n",
       "3         54800     172.217.7.2                443          6   \n",
       "4         54801   172.217.10.98                443          6   \n",
       "\n",
       "                    Timestamp   Flow Duration   Total Fwd Packets  ...  \\\n",
       "0  2018-11-03 09:18:16.964447       114456999                  45  ...   \n",
       "1  2018-11-03 09:18:18.506537       114347504                  56  ...   \n",
       "2  2018-11-03 09:18:18.610576        36435473                   6  ...   \n",
       "3  2018-11-03 09:18:18.610579        36434705                   6  ...   \n",
       "4  2018-11-03 09:18:18.610581        36434626                   6  ...   \n",
       "\n",
       "      Active Std   Active Max   Active Min    Idle Mean       Idle Std  \\\n",
       "0   28337.112288      98168.0          3.0   9529897.25  351582.631269   \n",
       "1  121314.911865     420255.0          4.0   9493929.75  351541.079539   \n",
       "2       0.000000      62416.0      62416.0  36373056.00       0.000000   \n",
       "3       0.000000      62413.0      62413.0  36372291.00       0.000000   \n",
       "4       0.000000      62409.0      62409.0  36372216.00       0.000000   \n",
       "\n",
       "     Idle Max    Idle Min  SimillarHTTP   Inbound   Label  \n",
       "0  10001143.0   9048097.0             0         0  BENIGN  \n",
       "1   9978130.0   8820294.0             0         0  BENIGN  \n",
       "2  36373056.0  36373056.0             0         0  BENIGN  \n",
       "3  36372291.0  36372291.0             0         0  BENIGN  \n",
       "4  36372216.0  36372216.0             0         0  BENIGN  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1zvZB0tfSSyF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Flow ID', ' Source IP', ' Source Port',\n",
       "       ' Destination IP', ' Destination Port', ' Protocol', ' Timestamp',\n",
       "       ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
       "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
       "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
       "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
       "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
       "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
       "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
       "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
       "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
       "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
       "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
       "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
       "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
       "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
       "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
       "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
       "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
       "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
       "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
       "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
       "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
       "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
       "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
       "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
       "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
       "       ' Idle Max', ' Idle Min', 'SimillarHTTP', ' Inbound', ' Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Am30-F0PhEXd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.50.254</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>114456999</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.50.253</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>114347504</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>54799</td>\n",
       "      <td>172.217.10.98</td>\n",
       "      <td>443</td>\n",
       "      <td>36435473</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>54800</td>\n",
       "      <td>172.217.7.2</td>\n",
       "      <td>443</td>\n",
       "      <td>36434705</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.50.6</td>\n",
       "      <td>54801</td>\n",
       "      <td>172.217.10.98</td>\n",
       "      <td>443</td>\n",
       "      <td>36434626</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source IP   Source Port  Destination IP   Destination Port  \\\n",
       "0  192.168.50.254             0       224.0.0.5                  0   \n",
       "1  192.168.50.253             0       224.0.0.5                  0   \n",
       "2    192.168.50.6         54799   172.217.10.98                443   \n",
       "3    192.168.50.6         54800     172.217.7.2                443   \n",
       "4    192.168.50.6         54801   172.217.10.98                443   \n",
       "\n",
       "    Flow Duration   Total Fwd Packets   Total Backward Packets  \\\n",
       "0       114456999                  45                        0   \n",
       "1       114347504                  56                        0   \n",
       "2        36435473                   6                        2   \n",
       "3        36434705                   6                        2   \n",
       "4        36434626                   6                        2   \n",
       "\n",
       "   Total Length of Fwd Packets   Total Length of Bwd Packets   Label  \n",
       "0                          0.0                           0.0       0  \n",
       "1                          0.0                           0.0       0  \n",
       "2                        116.0                          92.0       0  \n",
       "3                        116.0                          92.0       0  \n",
       "4                        116.0                          92.0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[' Source IP', ' Source Port',' Destination IP', ' Destination Port',\n",
    "       ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
    "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',' Label']]\n",
    "\n",
    "# Map labels in data_1 to binary\n",
    "data[' Label'] = data[' Label'].map({\n",
    "    'BENIGN': 0,\n",
    "    'Portmap': 1})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ETVxFV9obG4U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    186960\n",
       "0      4734\n",
       "Name:  Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ciHA5fClpP6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in the balanced data: 2\n",
      "Frequency of each unique label in the balanced data:\n",
      "1    150000\n",
      "0    150000\n",
      "Name:  Label, dtype: int64\n",
      "Frequency of each unique label in the original data:\n",
      "1    186960\n",
      "0      4734\n",
      "Name:  Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Set the desired number of majority class samples\n",
    "num_majority_samples = 150000  # Adjust this number as needed\n",
    "\n",
    "# Get the minority class label\n",
    "minority_class = data[' Label'].value_counts().idxmin()\n",
    "\n",
    "# Separate majority and minority class samples\n",
    "majority_samples = data[data[' Label'] != minority_class]\n",
    "minority_samples = data[data[' Label'] == minority_class]\n",
    "\n",
    "# Sample the majority class samples\n",
    "majority_samples_sampled = majority_samples.sample(num_majority_samples, random_state=42)\n",
    "\n",
    "# Upsample the minority class samples\n",
    "minority_samples_upsampled = resample(\n",
    "    minority_samples,\n",
    "    replace=True,  # Sample with replacement\n",
    "    n_samples=num_majority_samples,  # Match the number of majority samples\n",
    "    random_state=42  # Reproducible results\n",
    ")\n",
    "# Combine the sampled majority class samples and upsampled minority class samples\n",
    "balanced_data = pd.concat([majority_samples_sampled, minority_samples_upsampled], axis=0)\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Print some information about the balanced dataset\n",
    "balanced_data.head()\n",
    "num_unique_labels = balanced_data[' Label'].nunique()\n",
    "print(\"Number of unique labels in the balanced data:\", num_unique_labels)\n",
    "label_frequency = balanced_data[' Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the balanced data:\")\n",
    "print(label_frequency)\n",
    "label_frequency_data = data[' Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the original data:\")\n",
    "print(label_frequency_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NzNu3IMeaYH-"
   },
   "outputs": [],
   "source": [
    "data=balanced_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3n-wgNGMWH3",
    "tags": []
   },
   "source": [
    "## Dataset2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UeX0y2rTACvI",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
       "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
       "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
       "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
       "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
       "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
       "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
       "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Cd0L_ETF4p_B"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>90909.090200</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1762</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.000300</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.005100</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>166666.660800</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "      <td>100000.002500</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>21.025787</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>21.987556</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409322</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>18.112692</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496889</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>18.445950</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0.487912</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>21.987556</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>REQ</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409322</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id        dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
       "0      1   0.000011   udp       -   INT      2      0     496       0   \n",
       "1      2   0.000008   udp       -   INT      2      0    1762       0   \n",
       "2      3   0.000005   udp       -   INT      2      0    1068       0   \n",
       "3      4   0.000006   udp       -   INT      2      0     900       0   \n",
       "4      5   0.000010   udp       -   INT      2      0    2126       0   \n",
       "..   ...        ...   ...     ...   ...    ...    ...     ...     ...   \n",
       "195  196  21.025787   tcp       -   REQ     10      0     450       0   \n",
       "196  197  21.987556   tcp       -   REQ     10      0     450       0   \n",
       "197  198  18.112692   tcp       -   REQ     10      0     450       0   \n",
       "198  199  18.445950   tcp       -   REQ     10      0     450       0   \n",
       "199  200  21.987556   tcp       -   REQ     10      0     450       0   \n",
       "\n",
       "              rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "0     90909.090200  ...                 1               2             0   \n",
       "1    125000.000300  ...                 1               2             0   \n",
       "2    200000.005100  ...                 1               3             0   \n",
       "3    166666.660800  ...                 1               3             0   \n",
       "4    100000.002500  ...                 1               3             0   \n",
       "..             ...  ...               ...             ...           ...   \n",
       "195       0.428046  ...                 2              25             0   \n",
       "196       0.409322  ...                 2              25             0   \n",
       "197       0.496889  ...                 2              25             0   \n",
       "198       0.487912  ...                 1              25             0   \n",
       "199       0.409322  ...                 2              25             0   \n",
       "\n",
       "     ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
       "0             0                 0           1           2                0   \n",
       "1             0                 0           1           2                0   \n",
       "2             0                 0           1           3                0   \n",
       "3             0                 0           2           3                0   \n",
       "4             0                 0           2           3                0   \n",
       "..          ...               ...         ...         ...              ...   \n",
       "195           0                 0          22          25                0   \n",
       "196           0                 0          22          25                0   \n",
       "197           0                 0          22          25                0   \n",
       "198           0                 0          22          25                0   \n",
       "199           0                 0          22          25                0   \n",
       "\n",
       "     attack_cat  label  \n",
       "0        Normal      0  \n",
       "1        Normal      0  \n",
       "2        Normal      0  \n",
       "3        Normal      0  \n",
       "4        Normal      0  \n",
       "..          ...    ...  \n",
       "195      Normal      0  \n",
       "196      Normal      0  \n",
       "197      Normal      0  \n",
       "198      Normal      0  \n",
       "199      Normal      0  \n",
       "\n",
       "[200 rows x 45 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mMKKWtauAN-z"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "data_2['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oaEigChD5lZ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Reconnaissance', 'Backdoor', 'Shellcode', 'Analysis', 'DoS', 'Exploits', 'Generic', 'Fuzzers', 'Worms', 'Normal'}\n",
      "Normal    37000\n",
      "DoS        4089\n",
      "Name: attack_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print unique values in 'attack_cat' column\n",
    "print(set(data_2['attack_cat']))\n",
    "\n",
    "# Replace NaN values with 'Normal'\n",
    "data_2['attack_cat'].fillna('Normal', inplace=True)\n",
    "\n",
    "# Filter data_2 to keep only 'DoS' and 'Normal' rows\n",
    "data_2 = data_2[data_2['attack_cat'].isin(['DoS', 'Normal'])]\n",
    "\n",
    "# Verify the filtering\n",
    "print(data_2['attack_cat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NaFrbWLxfbm-"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['srcip', 'sport', 'dstip', 'dsport', 'Spkts', 'Dpkts', 'Label'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_2\u001b[38;5;241m=\u001b[39m \u001b[43mdata_2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrcip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdstip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdsport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdur\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpkts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDpkts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msbytes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdbytes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m data_2\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['srcip', 'sport', 'dstip', 'dsport', 'Spkts', 'Dpkts', 'Label'] not in index\""
     ]
    }
   ],
   "source": [
    "data_2= data_2[['srcip', 'sport', 'dstip', 'dsport','dur',  'Spkts', 'Dpkts', 'sbytes',\n",
    "       'dbytes', 'Label']]\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcYCffhU32nC"
   },
   "outputs": [],
   "source": [
    "data_2['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubkGgDbgO_j5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Set the desired number of majority class samples\n",
    "num_majority_samples = 15000  # Adjust this number as needed\n",
    "\n",
    "# Get the minority class label\n",
    "minority_class = data_2['Label'].value_counts().idxmin()\n",
    "\n",
    "# Separate majority and minority class samples\n",
    "majority_samples = data_2[data_2['Label'] != minority_class]\n",
    "minority_samples = data_2[data_2['Label'] == minority_class]\n",
    "\n",
    "# Sample the majority class samples\n",
    "majority_samples_sampled = majority_samples.sample(num_majority_samples, random_state=42)\n",
    "\n",
    "# Upsample the minority class samples\n",
    "minority_samples_upsampled = resample(\n",
    "    minority_samples,\n",
    "    replace=True,  # Sample with replacement\n",
    "    n_samples=num_majority_samples,  # Match the number of majority samples\n",
    "    random_state=42  # Reproducible results\n",
    ")\n",
    "\n",
    "# Combine the sampled majority class samples and upsampled minority class samples\n",
    "balanced_data_2 = pd.concat([majority_samples_sampled, minority_samples_upsampled], axis=0)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data_2 = balanced_data_2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print some information about the balanced dataset\n",
    "balanced_data.head()\n",
    "num_unique_labels = balanced_data_2['Label'].nunique()\n",
    "print(\"Number of unique labels in the balanced data:\", num_unique_labels)\n",
    "label_frequency = balanced_data_2['Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the balanced data:\")\n",
    "print(label_frequency)\n",
    "label_frequency_data = data_2['Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the original data:\")\n",
    "print(label_frequency_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY28Y2JfO__L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2 =  balanced_data_2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOuY2pwhMc6G",
    "tags": []
   },
   "source": [
    "## Dataset3 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhVnzDNiMgO3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny_qGk3iP2nF"
   },
   "outputs": [],
   "source": [
    "data_3 = data_3[[' Source IP', ' Source Port', ' Destination IP',\n",
    "       ' Destination Port',' Flow Duration',\n",
    "       ' Total Fwd Packets', ' Total Backward Packets',\n",
    "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',' Label']]\n",
    "\n",
    "data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gt5Y5TPAQuKg"
   },
   "outputs": [],
   "source": [
    "print(set(data_3[' Label']))\n",
    "\n",
    "data_3[' Label'].value_counts()\n",
    "# Map labels in data_1 to binary\n",
    "data_3[' Label'] = data_3[' Label'].map({\n",
    "    'BENIGN': 0,\n",
    "    'DDoS': 1})\n",
    "\n",
    "data_3[' Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xgu8cObb51PX"
   },
   "outputs": [],
   "source": [
    "data_3[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdcxcB6yifiO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Set the desired number of majority class samples\n",
    "num_majority_samples = 15000  # Adjust this number as needed\n",
    "\n",
    "# Get the minority class label\n",
    "minority_class = data_3[' Label'].value_counts().idxmin()\n",
    "\n",
    "# Separate majority and minority class samples\n",
    "majority_samples = data_3[data_3[' Label'] != minority_class]\n",
    "minority_samples = data_3[data_3[' Label'] == minority_class]\n",
    "\n",
    "# Sample the majority class samples\n",
    "majority_samples_sampled = majority_samples.sample(num_majority_samples, random_state=42)\n",
    "\n",
    "# Upsample the minority class samples\n",
    "minority_samples_upsampled = resample(\n",
    "    minority_samples,\n",
    "    replace=True,  # Sample with replacement\n",
    "    n_samples=num_majority_samples,  # Match the number of majority samples\n",
    "    random_state=42  # Reproducible results\n",
    ")\n",
    "\n",
    "# Combine the sampled majority class samples and upsampled minority class samples\n",
    "balanced_data_3 = pd.concat([majority_samples_sampled, minority_samples_upsampled], axis=0)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data_3 = balanced_data_3.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print some information about the balanced dataset\n",
    "balanced_data.head()\n",
    "num_unique_labels = balanced_data_3[' Label'].nunique()\n",
    "print(\"Number of unique labels in the balanced data:\", num_unique_labels)\n",
    "label_frequency = balanced_data_3[' Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the balanced data:\")\n",
    "print(label_frequency)\n",
    "label_frequency_data = data_3[' Label'].value_counts()\n",
    "print(\"Frequency of each unique label in the original data:\")\n",
    "print(label_frequency_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4TpOjXpj5xd"
   },
   "outputs": [],
   "source": [
    "data_3 = balanced_data_3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNnplKV5MmZi",
    "tags": []
   },
   "source": [
    "## Standardizing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxDhrs85RIch"
   },
   "outputs": [],
   "source": [
    "data.columns,data_2.columns,data_3.columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIyTViFfTWtZ"
   },
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "rename_dict = {\n",
    "    ' Source IP': 'src_ip', ' Source Port': 'src_port', ' Destination IP': 'dst_ip', ' Destination Port': 'dst_port',\n",
    "    ' Protocol': 'protocol', ' Flow Duration': 'flow_duration', ' Total Fwd Packets': 'total_fwd_packets',\n",
    "    ' Total Backward Packets': 'total_bwd_packets', 'Total Length of Fwd Packets': 'total_len_fwd_packets',\n",
    "    ' Total Length of Bwd Packets': 'total_len_bwd_packets', ' Label': 'label',\n",
    "\n",
    "    'srcip': 'src_ip', 'sport': 'src_port', 'dstip': 'dst_ip', 'dsport': 'dst_port',\n",
    "    'proto': 'protocol', 'dur': 'flow_duration', 'Spkts': 'total_fwd_packets', 'Dpkts': 'total_bwd_packets',\n",
    "    'sbytes': 'total_len_fwd_packets', 'dbytes': 'total_len_bwd_packets', 'Label': 'label'\n",
    "}\n",
    "\n",
    "data.rename(columns=rename_dict, inplace=True)\n",
    "data_2.rename(columns=rename_dict, inplace=True)\n",
    "data_3.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA_6Oi0HTfiR"
   },
   "outputs": [],
   "source": [
    "data.columns,data_2.columns,data_3.columns,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_MGSeFEM38Z",
    "tags": []
   },
   "source": [
    "## Preparing Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErMV4UYTYvGj"
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "data.fillna(0, inplace=True)\n",
    "data_2.fillna(0, inplace=True)\n",
    "data_3.fillna(0, inplace=True)\n",
    "\n",
    "# Combine the datasets vertically\n",
    "combined_data = pd.concat([data,data_2, data_3], ignore_index=True)\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUo4LpLQZDjk"
   },
   "outputs": [],
   "source": [
    "combined_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9fPtaTYX_Iq"
   },
   "outputs": [],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SMk4Y1kjcge"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# Create separate LabelEncoders for each categorical feature\n",
    "encoder_src_ip = LabelEncoder()\n",
    "encoder_dst_ip = LabelEncoder()\n",
    "encoder_src_port = LabelEncoder()\n",
    "encoder_dst_port = LabelEncoder()\n",
    "\n",
    "encoder_label = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "combined_data['src_ip'] = encoder_src_ip.fit_transform(combined_data['src_ip'])\n",
    "combined_data['dst_ip'] = encoder_dst_ip.fit_transform(combined_data['dst_ip'])\n",
    "combined_data['src_port'] = encoder_src_port.fit_transform(combined_data['src_port'])\n",
    "combined_data['dst_port'] = encoder_dst_port.fit_transform(combined_data['dst_port'].astype(str))\n",
    "\n",
    "combined_data['label'] = encoder_label.fit_transform(combined_data['label'])\n",
    "\n",
    "# Save the encoders\n",
    "with open('encoder_src_ip.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder_src_ip, f)\n",
    "\n",
    "with open('encoder_dst_ip.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder_dst_ip, f)\n",
    "# Save the encoders\n",
    "with open('encoder_src_port.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder_src_port, f)\n",
    "\n",
    "with open('encoder_dst_port.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder_dst_port, f)\n",
    "\n",
    "with open('encoder_label.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder_label, f)\n",
    "\n",
    "print(\"Encoders saved successfully!\")\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s26T5AoCs-FD"
   },
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKXfXqVKm83F"
   },
   "outputs": [],
   "source": [
    "combined_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hly-X2lxty9k"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "\n",
    "X = combined_data.drop('label', axis=1)\n",
    "y = combined_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khLWdYqZ78lu"
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9Ssmjv1t5_E"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DarY50-t9TA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to check for data issues\n",
    "def check_data_issues(data):\n",
    "    if data.isnull().values.any():\n",
    "        print(\"There are NaN values in the dataset.\")\n",
    "    else:\n",
    "        print(\"No NaN values found in the dataset.\")\n",
    "\n",
    "    if np.isinf(data).values.any():\n",
    "        print(\"There are infinite values in the dataset.\")\n",
    "    else:\n",
    "        print(\"No infinite values found in the dataset.\")\n",
    "\n",
    "    max_value = data.max().max()\n",
    "    if max_value > np.finfo(np.float64).max:\n",
    "        print(f\"There are values too large for dtype('float64') in the dataset. Max value: {max_value}\")\n",
    "    else:\n",
    "        print(\"No values too large for dtype('float64') found in the dataset.\")\n",
    "\n",
    "    min_value = data.min().min()\n",
    "    if min_value < np.finfo(np.float64).min:\n",
    "        print(f\"There are values too small for dtype('float64') in the dataset. Min value: {min_value}\")\n",
    "    else:\n",
    "        print(\"No values too small for dtype('float64') found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siqmSmeV615R"
   },
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = X_train.select_dtypes(exclude=[np.number]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvnfqNTI63x0"
   },
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_columns,categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXKEKw8guSJj"
   },
   "outputs": [],
   "source": [
    "# Replace infinity values with NaN\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Impute NaN values with the mean of the corresponding column\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Now, standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfnrV4BCt9nH"
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "\n",
    "# Check the training and testing datasets for issues\n",
    "print(\"Checking X_train:\")\n",
    "check_data_issues(X_train)\n",
    "print(\"\\nChecking X_test:\")\n",
    "check_data_issues(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW2E-Hyy6RhG"
   },
   "outputs": [],
   "source": [
    "X_train.columns\n",
    "column_names =X_train.columns\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlpIcXMOvUX6"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTWVrQXmgA2r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump\n",
    "\n",
    "# Create an empty DataFrame with the desired columns\n",
    "model_comparison = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "saved_models = {}  # Dictionary to store saved models\n",
    "\n",
    "\n",
    "# Train and evaluate machine learning models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC()\n",
    "}\n",
    "print(\"X shape\",X_train.shape)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Append the metrics to the DataFrame\n",
    "    model_comparison = model_comparison._append({\"Model\": name, \"Accuracy\": accuracy, \"F1 Score\": f1}, ignore_index=True)\n",
    "\n",
    "    # Save the trained model weight\n",
    "    filename = f\"{name}_model.joblib\"\n",
    "    dump(model, filename)\n",
    "    saved_models[name] = filename  # Store the filename for later use in the tested DataSets\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Model saved as: {filename}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "display(model_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtvq0IxFsRSJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the DataFrame to store model comparison\n",
    "model_comparison = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "\n",
    "# Initialize dictionary to store confusion matrices\n",
    "confusion_matrices = {}\n",
    "\n",
    "# Initialize lists to store accuracy and F1 score for each model\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over each loaded model\n",
    "for name, model in models.items():\n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "\n",
    "    # Append results to model comparison DataFrame\n",
    "    model_comparison = model_comparison._append({\"Model\": name, \"Accuracy\": accuracy, \"F1 Score\": f1}, ignore_index=True)\n",
    "\n",
    "    # Append accuracy and F1 score to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print model evaluation metrics and confusion matrix\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plotting accuracy scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(model_comparison['Model'], accuracy_scores, color='skyblue')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plotting F1 scores\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(model_comparison['Model'], f1_scores, color='orange')\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Displaying the model comparison DataFrame\n",
    "print(\"Model Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Plot confusion matrices for each model using Seaborn\n",
    "for name, cm in confusion_matrices.items():\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZP--5ZNSlLw",
    "tags": []
   },
   "source": [
    "# Loading Saved model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FryA8a9igzO"
   },
   "outputs": [],
   "source": [
    "#Load the saved model\n",
    "from joblib import load\n",
    "\n",
    "# Load each model individually\n",
    "# Give path of each model here\n",
    "logistic_regression_model = load(\"Logistic Regression_model.joblib\")\n",
    "random_forest_model = load(\"Random Forest_model.joblib\")\n",
    "svm_model = load(\"Support Vector Machine_model.joblib\")\n",
    "\n",
    "# Store the loaded models in a dictionary\n",
    "loaded_models = {\n",
    "    \"Logistic Regression\": logistic_regression_model,\n",
    "    \"Random Forest\": random_forest_model,\n",
    "    \"Support Vector Machine\": svm_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrKkHH-11HZK",
    "outputId": "e2c7bf3f-f6e1-48e3-c265-5cfb61d6a11f"
   },
   "outputs": [],
   "source": [
    "loaded_models.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "yn6NKiPgkjol",
    "outputId": "13dd7107-e312-4cef-c1a1-d07a92335b58"
   },
   "outputs": [],
   "source": [
    "# Load the encoders\n",
    "with open('encoder_src_ip.pkl', 'rb') as f:\n",
    "    loaded_encoder_src_ip = pickle.load(f)\n",
    "\n",
    "with open('encoder_dst_ip.pkl', 'rb') as f:\n",
    "    loaded_encoder_dst_ip = pickle.load(f)\n",
    "\n",
    "with open('encoder_label.pkl', 'rb') as f:\n",
    "    loaded_encoder_label = pickle.load(f)\n",
    "\n",
    "print(\"Encoders loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nitxogSZ1yzg"
   },
   "source": [
    "##Preprocessing For Testing DATA Change Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "tGNzsOc3ujsD",
    "outputId": "0b7bcfd3-0bfe-4f48-d9da-0fa1c89861e0"
   },
   "outputs": [],
   "source": [
    "feature_columns = X_train.columns\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENgTyePU2MDI"
   },
   "outputs": [],
   "source": [
    "column_mapping = {'Flow ID': 'Flow ID',\n",
    " 'Src IP': ' Source IP',\n",
    " 'Src Port': ' Source Port',\n",
    " 'Dst IP': ' Destination IP',\n",
    " 'Dst Port': ' Destination Port',\n",
    " 'Protocol': ' Protocol',\n",
    " 'Timestamp': ' Timestamp',\n",
    " 'Flow Duration': ' Flow Duration',\n",
    " 'Tot Fwd Pkts': ' Total Fwd Packets',\n",
    " 'Tot Bwd Pkts': ' Total Backward Packets',\n",
    " 'TotLen Fwd Pkts': 'Total Length of Fwd Packets',\n",
    " 'TotLen Bwd Pkts': ' Total Length of Bwd Packets',\n",
    " 'Fwd Pkt Len Max': ' Fwd Packet Length Max',\n",
    " 'Fwd Pkt Len Min': ' Fwd Packet Length Min',\n",
    " 'Fwd Pkt Len Mean': ' Fwd Packet Length Mean',\n",
    " 'Fwd Pkt Len Std': ' Fwd Packet Length Std',\n",
    " 'Bwd Pkt Len Max': 'Bwd Packet Length Max',\n",
    " 'Bwd Pkt Len Min': ' Bwd Packet Length Min',\n",
    " 'Bwd Pkt Len Mean': ' Bwd Packet Length Mean',\n",
    " 'Bwd Pkt Len Std': ' Bwd Packet Length Std',\n",
    " 'Flow Byts/s': 'Flow Bytes/s',\n",
    " 'Flow Pkts/s': ' Flow Packets/s',\n",
    " 'Flow IAT Mean': ' Flow IAT Mean',\n",
    " 'Flow IAT Std': ' Flow IAT Std',\n",
    " 'Flow IAT Max': ' Flow IAT Max',\n",
    " 'Flow IAT Min': ' Flow IAT Min',\n",
    " 'Fwd IAT Tot': 'Fwd IAT Total',\n",
    " 'Fwd IAT Mean': ' Fwd IAT Mean',\n",
    " 'Fwd IAT Std': ' Fwd IAT Std',\n",
    " 'Fwd IAT Max': ' Fwd IAT Max',\n",
    " 'Fwd IAT Min': ' Fwd IAT Min',\n",
    " 'Bwd IAT Tot': 'Bwd IAT Total',\n",
    " 'Bwd IAT Mean': ' Bwd IAT Mean',\n",
    " 'Bwd IAT Std': ' Bwd IAT Std',\n",
    " 'Bwd IAT Max': ' Bwd IAT Max',\n",
    " 'Bwd IAT Min': ' Bwd IAT Min',\n",
    " 'Fwd PSH Flags': 'Fwd PSH Flags',\n",
    " 'Bwd PSH Flags': ' Bwd PSH Flags',\n",
    " 'Fwd URG Flags': ' Fwd URG Flags',\n",
    " 'Bwd URG Flags': ' Bwd URG Flags',\n",
    " 'Fwd Header Len': ' Fwd Header Length',\n",
    " 'Bwd Header Len': ' Bwd Header Length',\n",
    " 'Fwd Pkts/s': 'Fwd Packets/s',\n",
    " 'Bwd Pkts/s': ' Bwd Packets/s',\n",
    " 'Pkt Len Min': ' Min Packet Length',\n",
    " 'Pkt Len Max': ' Max Packet Length',\n",
    " 'Pkt Len Mean': ' Packet Length Mean',\n",
    " 'Pkt Len Std': ' Packet Length Std',\n",
    " 'Pkt Len Var': ' Packet Length Variance',\n",
    " 'FIN Flag Cnt': 'FIN Flag Count',\n",
    " 'SYN Flag Cnt': ' SYN Flag Count',\n",
    " 'RST Flag Cnt': ' RST Flag Count',\n",
    " 'PSH Flag Cnt': ' PSH Flag Count',\n",
    " 'ACK Flag Cnt': ' ACK Flag Count',\n",
    " 'URG Flag Cnt': ' URG Flag Count',\n",
    " 'CWE Flag Count': ' CWE Flag Count',\n",
    " 'ECE Flag Cnt': ' ECE Flag Count',\n",
    " 'Down/Up Ratio': ' Down/Up Ratio',\n",
    " 'Pkt Size Avg': ' Average Packet Size',\n",
    " 'Fwd Seg Size Avg': ' Avg Fwd Segment Size',\n",
    " 'Bwd Seg Size Avg': ' Avg Bwd Segment Size',\n",
    " 'Fwd Byts/b Avg': 'Fwd Avg Bytes/Bulk',\n",
    " 'Fwd Pkts/b Avg': ' Fwd Avg Packets/Bulk',\n",
    " 'Fwd Blk Rate Avg': ' Fwd Avg Bulk Rate',\n",
    " 'Bwd Byts/b Avg': ' Bwd Avg Bytes/Bulk',\n",
    " 'Bwd Pkts/b Avg': ' Bwd Avg Packets/Bulk',\n",
    " 'Bwd Blk Rate Avg': 'Bwd Avg Bulk Rate',\n",
    " 'Subflow Fwd Pkts': 'Subflow Fwd Packets',\n",
    " 'Subflow Fwd Byts': ' Subflow Fwd Bytes',\n",
    " 'Subflow Bwd Pkts': ' Subflow Bwd Packets',\n",
    " 'Subflow Bwd Byts': ' Subflow Bwd Bytes',\n",
    " 'Init Fwd Win Byts': 'Init_Win_bytes_forward',\n",
    " 'Init Bwd Win Byts': ' Init_Win_bytes_backward',\n",
    " 'Fwd Act Data Pkts': ' act_data_pkt_fwd',\n",
    " 'Fwd Seg Size Min': ' min_seg_size_forward',\n",
    " 'Active Mean': 'Active Mean',\n",
    " 'Active Std': ' Active Std',\n",
    " 'Active Max': ' Active Max',\n",
    " 'Active Min': ' Active Min',\n",
    " 'Idle Mean': 'Idle Mean',\n",
    " 'Idle Std': ' Idle Std',\n",
    " 'Idle Max': ' Idle Max',\n",
    " 'Idle Min': ' Idle Min',\n",
    " 'Label': 'Label'}\n",
    "\n",
    " # Function to standardize column names\n",
    "def standardize_columns(df, column_mapping):\n",
    "    # df = df.rename(columns=column_mapping)\n",
    "    if ' Label' in df.columns:\n",
    "        df = df.rename(columns={' Label': 'Label'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-qMOEq4Ed-J"
   },
   "outputs": [],
   "source": [
    "specific_columns = [' Source IP', ' Source Port',' Destination IP', ' Destination Port',\n",
    "       ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
    "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets','Label']\n",
    "\n",
    "# Standardize column names\n",
    "rename_dict = {\n",
    "    ' Source IP': 'src_ip', ' Source Port': 'src_port', ' Destination IP': 'dst_ip', ' Destination Port': 'dst_port',\n",
    "    ' Protocol': 'protocol', ' Flow Duration': 'flow_duration', ' Total Fwd Packets': 'total_fwd_packets',\n",
    "    ' Total Backward Packets': 'total_bwd_packets', 'Total Length of Fwd Packets': 'total_len_fwd_packets',\n",
    "    ' Total Length of Bwd Packets': 'total_len_bwd_packets', ' Label': 'label',\n",
    "\n",
    "    'srcip': 'src_ip', 'sport': 'src_port', 'dstip': 'dst_ip', 'dsport': 'dst_port',\n",
    "    'proto': 'protocol', 'dur': 'flow_duration', 'Spkts': 'total_fwd_packets', 'Dpkts': 'total_bwd_packets',\n",
    "    'sbytes': 'total_len_fwd_packets', 'dbytes': 'total_len_bwd_packets', 'Label': 'label'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D7xkdrRRujD",
    "tags": []
   },
   "source": [
    "# Loading target dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8rGrRtwi0uM",
    "outputId": "9ccf315b-1cd0-46ee-f5c3-c3d4ec5e850f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the new dataset we going to test the model on:\n",
    "url = \"https://ddosciu.s3.us-east-2.amazonaws.com/PCAPs/Test.csv\"\n",
    "#Test1\n",
    "url1 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/HTTP_Ddos.pcap_Flow.csv\"\n",
    "url2 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.TCP.syn.optionallyACK.optionallysamePort.pcapng_Flow.csv\"\n",
    "url3 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.TCP.reflection.SYNACK.pcap_Flow.csv\"\n",
    "url4 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.UDP.bacnet.IOT.37810.pcapng_Flow.csv\"\n",
    "url5 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/1.pcap_Flow.csv\"\n",
    "url6 =\"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.dns.RRSIG.fragmented.pcap_Flow.csv\"\n",
    "url7 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/amp.UDP.isakmp.pcap_Flow.csv\"\n",
    "url8 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/pkt.ICMP.largeempty.pcap_Flow.csv\"\n",
    "url9 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/pkt.TCP.DOMINATE.syn.ecn.cwr.pcapng_Flow.csv\"\n",
    "url10 = \"/content/NormalTraffic2.pcap_Flow.csv\"\n",
    "url11= \"https://ddosciu.s3.us-east-2.amazonaws.com/IC_Taraf.root.1.pcap_Flow.csv\"\n",
    "url12 = \"/content/drive/MyDrive/online_tasks/Portmap.csv\"\n",
    "url13 = \"https://ddosciu.s3.us-east-2.amazonaws.com/CSV's/Train.csv\"\n",
    "url14 = \"/content/drive/MyDrive/online_tasks/UNSW-NB15_4.csv\"\n",
    "url15 = \"/content/drive/MyDrive/online_tasks/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
    "url16 = \"https://ddosciu.s3.us-east-2.amazonaws.com/Portmap.csv\"\n",
    "new_data = pd.read_csv(url16)\n",
    "\n",
    "# Apply the column mapping to the testing data\n",
    "new_data = standardize_columns(new_data, column_mapping)\n",
    "\n",
    "new_data = new_data.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "T6TR_h2Zp-4I",
    "outputId": "583cb591-6453-46b5-af85-bdec3d8e1e57"
   },
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cW1WNCBjE8zh",
    "outputId": "2aaa28b2-ea4d-4f70-b5d9-93162ecee649"
   },
   "outputs": [],
   "source": [
    "new_data = new_data[specific_columns]\n",
    "new_data.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqceS9yXcfUA",
    "outputId": "ec870eb7-3db3-481e-b4ec-108e46e0eb7e"
   },
   "outputs": [],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osEj0ittA4Ts",
    "outputId": "24468477-e723-4490-dfb7-395581c50394"
   },
   "outputs": [],
   "source": [
    "len(X.columns),len(new_data.columns)\n",
    "# Check for missing columns in new_data compared to train data X\n",
    "missing_columns = set(X.columns) - set(new_data.columns)\n",
    "\n",
    "# Check for missing columns in train data X compared to new_data\n",
    "extra_columns = set(new_data.columns) - set(X.columns)\n",
    "\n",
    "print(\"Columns missing in new_data:\", missing_columns)\n",
    "print(\"Columns extra in new_data:\", extra_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "6S-vpcNyCqcl",
    "outputId": "5abb58ff-afa3-4a73-db94-427fbae2b5f4"
   },
   "outputs": [],
   "source": [
    "new_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "nFH_Hg9zmDY3",
    "outputId": "020b9cc6-042b-4a57-8665-70b6ed8db052"
   },
   "outputs": [],
   "source": [
    "# Create separate LabelEncoders for each categorical feature\n",
    "encoder_src_ip = LabelEncoder()\n",
    "encoder_dst_ip = LabelEncoder()\n",
    "encoder_src_port = LabelEncoder()\n",
    "encoder_dst_port = LabelEncoder()\n",
    "\n",
    "encoder_label = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "new_data['src_ip'] = encoder_src_ip.fit_transform(new_data['src_ip'])\n",
    "new_data['dst_ip'] = encoder_dst_ip.fit_transform(new_data['dst_ip'])\n",
    "new_data['src_port'] = encoder_src_port.fit_transform(new_data['src_port'])\n",
    "new_data['dst_port'] = encoder_dst_port.fit_transform(new_data['dst_port'].astype(str))\n",
    "\n",
    "new_data['label'] = encoder_label.fit_transform(new_data['label'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xpQU2yfiNcb",
    "outputId": "2d3f5dba-66cc-46dd-e50f-b6b8da6837be"
   },
   "outputs": [],
   "source": [
    "set(new_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAUlYXvvtw9O",
    "outputId": "c3ee8de5-822f-45ca-9088-5bacabb79181"
   },
   "outputs": [],
   "source": [
    "# Inspect the mapping between original labels and encoded numbers\n",
    "print(\"Mapping between original labels and encoded numbers:\")\n",
    "for label, encoded_label in zip(encoder_label.classes_, encoder_label.transform(encoder_label.classes_)):\n",
    "    print(f\"{label}: {encoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hsdj1VWT_gtK"
   },
   "outputs": [],
   "source": [
    "X_new = new_data.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w3vyogQb8iE"
   },
   "outputs": [],
   "source": [
    "X_new.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_new = imputer.fit_transform(X_new)\n",
    "scaler = StandardScaler()\n",
    "X_new = scaler.fit_transform(X_new)\n",
    "\n",
    "#Labels for comparison (They are not always available for some datasets)\n",
    "y_new = new_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hc83C6eK_XjD",
    "outputId": "f87b8a18-261a-41ce-ad41-a331c9b262dc"
   },
   "outputs": [],
   "source": [
    "y_new.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz2fc-oDBfw_",
    "tags": []
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuYSrD_DranY",
    "outputId": "b31af589-4dd8-4d52-e128-c6c494e7e4a0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_new and y_new are already defined\n",
    "# Example:\n",
    "# X_new = ...\n",
    "# y_new = ...\n",
    "\n",
    "# Initialize an empty DataFrame for predictions\n",
    "all_predictions_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a list to store model names and accuracies\n",
    "model_names = []\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each loaded model\n",
    "for name, model in loaded_models.items():\n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_new)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_new, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Map the predicted labels\n",
    "    mapped_predictions = ['No Attack' if label == 0 else 'Attack' for label in y_pred]\n",
    "\n",
    "    # Add the model name to the list\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Create a DataFrame for predicted labels\n",
    "    predicted_labels_df = pd.DataFrame({'Predicted_Label': y_pred, 'Mapped_Prediction': mapped_predictions})\n",
    "\n",
    "    # Add the DataFrame to the overall DataFrame\n",
    "    all_predictions_df = pd.concat([all_predictions_df, predicted_labels_df], axis=1)\n",
    "\n",
    "# Set the column names for the first row\n",
    "all_predictions_df.columns = pd.MultiIndex.from_product([model_names, ['Predicted_Label', 'Mapped_Prediction']])\n",
    "\n",
    "# Create a DataFrame for accuracies\n",
    "accuracy_df = pd.DataFrame({'Model': model_names, 'Accuracy': accuracies})\n",
    "\n",
    "# Print the predictions DataFrame\n",
    "print(all_predictions_df)\n",
    "\n",
    "# Print the accuracies DataFrame\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FQ5vngRkrYUk",
    "outputId": "3e0cfef4-5655-4da6-b657-93a76b91bf3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the DataFrame to store model comparison\n",
    "model_comparison = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "\n",
    "# Initialize dictionary to store confusion matrices\n",
    "confusion_matrices = {}\n",
    "\n",
    "# Initialize lists to store accuracy and F1 score for each model\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over each loaded model\n",
    "for name, model in loaded_models.items():\n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_new)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_new, y_pred)\n",
    "    f1 = f1_score(y_new, y_pred, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_new, y_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "\n",
    "    # Append results to model comparison DataFrame\n",
    "    model_comparison = model_comparison._append({\"Model\": name, \"Accuracy\": accuracy, \"F1 Score\": f1}, ignore_index=True)\n",
    "\n",
    "    # Append accuracy and F1 score to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print model evaluation metrics and confusion matrix\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_new, y_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plotting accuracy scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(model_comparison['Model'], accuracy_scores, color='skyblue')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plotting F1 scores\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(model_comparison['Model'], f1_scores, color='orange')\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Displaying the model comparison DataFrame\n",
    "print(\"Model Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Plot confusion matrices for each model using Seaborn\n",
    "for name, cm in confusion_matrices.items():\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
